* A landscape scan of best practices for episodic labeling tools across mental health and time-series ML annotation (CLI, TUI, notebook, and web patterns)
* A proposed CLI interface design that aligns with your repo’s architecture (protobufs, Click, test structure)
* Data quality safeguards and labeling consistency mechanisms
* Extensible storage and version-control options for labeled data
* A focused 1-week roadmap to build and ship the MVP tool

I’ll include concrete examples, UX trade-offs, and implementation-ready suggestions. I’ll let you know when the report is ready.


# Executive Summary

Building a lightweight mood episode labeling tool for clinicians requires balancing usability with technical integration. We surveyed existing solutions in time-series annotation – from simple command-line interfaces (CLI) to interactive web dashboards – to identify best practices. Key findings include the importance of quick data visualization (to reduce errors), minimal setup (to accommodate busy clinicians), and robust audit trails (to trace label provenance). Based on our codebase and these insights, we recommend a **CLI-based labeling workflow** that leverages our existing pipeline (Python/Click), guiding the user through labeling daily timelines with episodes (Depression, Hypomania, Mania, or “No Episode” baseline). This CLI will prompt for date ranges, episode type, severity, and optional notes, performing validations (date formats, no overlaps, allowable episode types) and outputting labels in a structured JSON/CSV format ready for the fine-tuning pipeline. We also propose built-in quality checks (double-entry confirmation, optional confidence scores, conflict warnings) to improve label reliability. Labels will be persisted in version-controlled files (per-user JSON/CSV in a git-tracked `fixtures` folder, with a `--rater-id` for multi-rater extension) to seamlessly integrate with continuous integration (CI) and model training. Finally, we outline a one-week plan to implement the CLI: setting up CLI commands and tests, integrating with `EpisodeLabeler`, and defining success metrics (e.g. labeling speed, inter-rater agreement on sample data). The following sections provide a landscape comparison, detailed interface spec, data quality safeguards, extensibility considerations, and a development roadmap.

## Landscape Scan: Episodic Labeling Patterns in Time-Series Data

In both mental health and general ML contexts, tools for annotating time-series events have evolved in a few distinct patterns. We surveyed open-source projects and commercial tools that allow experts to label segments or events (“episodes”) in temporal data. **Table 1** summarizes 6 prevalent UI patterns, with examples and trade-offs for a clinician user.

| **Pattern & Example**                                                                    | **Interface**                                                 | **Pros (Clinician View)**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | **Cons / Trade-offs**                                                                                                                                                                                                                                                                                                                                                                                                      |
| ---------------------------------------------------------------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Manual Diaries / Spreadsheets** <br>*e.g. Paper charts, Excel (NIMH Life Chart)*       | Retrospective diary or spreadsheet; manual graph marking      | **Low setup** – no special software needed; familiar format (paper or Excel). <br>**Audit trail** – paper charts can be saved, Excel logs changes via file versions.                                                                                                                                                                                                                                                                                                                                                                       | **Very laborious** for large data – e.g. Excel could only plot 5k of 50k points, requiring manual lookup of timestamps to label each segment. <br>**High error risk** – fatigue and manual entry can introduce mistakes. <br>**Poor integration** – output not readily formatted for ML pipelines.                                                                                                                         |
| **Command-Line Prompt (CLI)** <br>*e.g. Custom Python scripts*                           | Text-based Q\&A or flags in terminal                          | **Fast input** (for keyboard-oriented users) – entering dates and labels directly can be efficient. <br>**No install friction** – runs in same environment as pipeline, no GUI needed. <br>**Easy audit** – CLI commands and outputs (files) are text, trackable in git.                                                                                                                                                                                                                                                                   | **No visual aid** – user must mentally map dates to events, increasing cognitive load. <br>**Potential input errors** – mistyping a date or label is possible (needs validation). <br>**Limited guidance** – without GUI, user must know commands or be guided by prompts.                                                                                                                                                 |
| **Text-Based UI (TUI)** <br>*e.g. Curses interface, Grafana in text mode*                | Interactive terminal UI (curses) or text-based dashboards     | **Visual context in terminal** – can present an ASCII timeline or summary stats, helping orientation. <br>**All-keyboard workflow** – suits fast typists, no mouse needed.                                                                                                                                                                                                                                                                                                                                                                 | **Complex to implement** – custom TUI for time-series (zoom/scroll) is non-trivial. <br>**Terminal limitations** – cannot match rich plotting of GUI; steep learning curve for new users. (No widely adopted open-source TUI specifically for time-series episodes was found).                                                                                                                                             |
| **Notebook Widget** <br>*e.g. QSL (Quick Simple Labeler), Pigeon*                        | Jupyter Notebook interactive widget (in-browser)              | **Rich visualization** – can plot time-series and let user select ranges with slider or mouse. QSL, for example, supports “point and range-based time-series labeling” inside Jupyter. <br>**Quick iteration** – ideal during data exploration; integrates with analysis code.                                                                                                                                                                                                                                                             | **Requires Jupyter environment** – not all clinicians are comfortable launching notebooks. <br>**Less standalone** – harder to package for non-coders (needs running Jupyter server). <br>**Moderate setup** – widget libraries must be installed and compatible with the environment.                                                                                                                                     |
| **Lightweight Web App** <br>*e.g. Geocene TRAINSET, Upalgo Labeling*                     | Web browser interface (local or hosted)                       | **Intuitive GUI** – point-and-click labeling on a timeline chart. TRAINSET lets users “brush labels onto time series data” and export a labeled CSV. Upalgo’s tool visualizes signals and even suggests segments to label, which the user can then refine and propagate to the whole file. <br>**Minimal coding** – user interacts via forms/buttons; multiple channels can be visualized. <br>**Collaboration** – a web app can be accessed by multiple reviewers (if hosted).                                                            | **Setup overhead** – may require running a local server or using a cloud service (which might conflict with data privacy for clinical data). <br>**Browser-based friction** – user must switch to a browser, which some command-line-oriented workflows may consider overhead. <br>**Versioning** – need to explicitly export labels; otherwise traceability relies on separate logs.                                      |
| **Full-Featured Annotation Platforms** <br>*e.g. Label Studio, Grafana with annotations* | Heavyweight web GUI or desktop app; supports multi-modal data | **Powerful features** – Label Studio (open-source) offers templates for time-series event classification with zoom/pan, multi-user support, and even model-driven labeling assists. Grafana, while primarily a monitoring tool, *“supports annotation”* on time-series charts and has an API to retrieve those labels. <br>**Audit & collaboration** – user management, comment threads, and an annotation database are often built-in. <br>**Data management** – can handle large datasets, version labels, and integrate with databases. | **Steep learning curve** – these tools have many options not needed for our use-case; clinicians would require training. <br>**Installation & maintenance** – running a server or app (plus dependencies like databases) is non-trivial for a quick labeling task. <br>**Overkill for MVP** – the extensive features (and complexity) may slow down immediate use; lightweight alternatives might achieve the goal faster. |

**Mental-Health Specific Considerations:** In psychiatric research, **episodic mood labeling** is often done via clinician interviews or patient self-reports recorded in mood charts. The **NIMH Life-Chart Method (LCM)**, for example, is a paper/electronic diary system where patients or clinicians record daily mood ratings and episode severity. This has become “*the de facto standard for long-term monitoring*” in bipolar disorder treatment. However, traditional diaries require manual transcription to be used in ML pipelines, and consistency can vary. Our tool can learn from these: providing a quick way to mark daily mood states (“No episode” vs episode type) similar to a life-chart, but directly outputting machine-readable labels. In summary, simpler interfaces (CLI/TUI) have low friction and can be integrated into research workflows easily, but they rely on user discipline to avoid errors. Rich GUIs offer visual safeguards (thus potentially lower error rate), but at the cost of setup complexity and potential adoption hurdles. For our clinician persona – who is comfortable running our pipeline in a terminal and values speed and simplicity – a **guided CLI** strikes the best balance initially. It avoids heavy installations and can log every action (for audit trail), while leaving the door open to a GUI or notebook extension later if needed.

## Recommended CLI Interface Specification

Leveraging our repository’s structure (the `mood-detector` CLI and `EpisodeLabeler` data model), we propose an interface that is intuitive in one-off use but also scriptable for power users. The CLI will operate as a new sub-command (or group of commands) under `mood-detector`. Below is the **specification of commands, flags, prompts, validation rules, output schema, and test integration**:

* **Command Structure:** We introduce a `mood-detector label` group with subcommands for different labeling tasks:

  * `mood-detector label episode` – Add a mood episode label.
  * `mood-detector label baseline` – Mark a stable (“no episode”) period.
  * `mood-detector label import` – Import labels from an external CSV/JSON (if clinicians have pre-recorded data).

  For brevity, `mood-detector label` on its own will default to the **episode** subcommand (since episode labeling is primary).

* **Flags & Arguments (Episode):**

  * `--date <YYYY-MM-DD>`: Label a single day as an episode.
  * `--date-range <YYYY-MM-DD:YYYY-MM-DD>`: Label a multi-day episode (inclusive of end date).
  * `--mood <category>`: Episode type, e.g. `"depressive"`, `"manic"`, `"hypomanic"`, `"mixed"`. (We will accept case-insensitive values or common abbreviations, mapping them to standard labels.)
  * `--severity <1-5>`: (Optional) Severity rating of the episode. If not provided, defaults to 3 (moderate) or a category-specific default. Severity is on a 1–5 scale as per our internal model expectations.
  * `--notes "<text>"`: (Optional) Free-text notes for context (e.g. “patient had decreased sleep, high energy”). This will be stored but not used by models, purely for record-keeping.
  * `--rater-id <ID>`: (Optional) Identifier for the clinician or rater (e.g. initials). Defaults to a configured user or “default”. This metadata will be saved with each label for future inter-rater analysis.

  **Examples:**

  ```bash
  # Label a single-day depressive episode (severity 3)
  mood-detector label --date 2025-03-15 --mood depressive --severity 3

  # Label a range as hypomanic episode (default severity)
  mood-detector label --date-range 2025-04-10:2025-04-17 --mood hypomanic

  # Mark a 2-week baseline period with a note
  mood-detector label baseline --start 2025-05-01 --end 2025-05-14 \
      --notes "Patient stable on medication"
  ```

* **Interactive Prompt Mode:** If the user runs `mood-detector label` without specifying the required flags, the CLI will enter an interactive Q\&A mode. It will prompt sequentially: *“Enter start date (YYYY-MM-DD, or press Enter for single date):”*, *“Enter end date (leave blank if single day):”*, *“Enter mood category:”*, *“Enter severity (1-5, default 3):”*, etc. This guided mode ensures even non-technical users can input labels without memorizing flags. Each prompt will have validation (e.g., date format and plausible range). This mode can also be triggered explicitly via a `--interactive` flag.

* **Validation Rules:** Robust input validation will prevent common errors:

  * **Date format & range:** Dates must be valid calendar dates. If a `--date-range` is given, the end must not precede the start; otherwise an error is shown. We’ll also check that dates don’t overlap with already labeled episodes in the current session (to avoid duplicates/overlaps unless explicitly intended).
  * **Mood category:** Must be one of the allowed categories (`depressive, manic, hypomanic, mixed`) or an alias (“none”/“baseline” allowed only in `label baseline` context). An invalid category triggers a list of valid options.
  * **Severity:** Must be integer 1–5. If a user inputs 0 or 6+, the CLI will reject and explain the scale. (We default severity=0 only for baseline labels to indicate no episode).
  * **Baseline vs Episode context:** The `label baseline` subcommand requires `--start` and `--end` flags (or interactive input) and **no** mood category (implicitly “baseline”). Conversely, `label episode` requires a mood. This separation avoids confusion between marking no-episode periods and actual episodes.
  * **Logical consistency:** If a user labels a date range as a certain episode and part of that range was previously marked baseline (or another episode) in the same session, the CLI will warn of overlap and prompt confirmation or adjustment. (This uses our `EpisodeLabeler` lists to check conflicts). Similarly, if two different mood episodes are entered with overlapping dates, a warning or prompt to split them is given.

* **Output JSON/CSV Schema:** After each labeling action (or batch of actions in interactive mode), the tool will update an output file capturing all labels. The default output location will be something like `labels/<USER_ID>_episodes.json` (plus a CSV if needed). The **JSON schema** will structure the data as follows for transparency and future parsing:

  ```json
  {
    "user_id": "USER123",
    "rater_id": "clinicianA",
    "episodes": [
      {
        "episode_type": "hypomanic",
        "start_date": "2025-04-10",
        "end_date": "2025-04-17",
        "severity": 3,
        "notes": "",
        "duration_days": 8
      },
      {
        "episode_type": "depressive",
        "start_date": "2025-03-15",
        "end_date": "2025-03-15",
        "severity": 3,
        "notes": "post-travel slump",
        "duration_days": 1
      }
    ],
    "baseline_periods": [
      {
        "start_date": "2025-05-01",
        "end_date": "2025-05-14",
        "notes": "Patient stable on meds",
        "duration_days": 14
      }
    ]
  }
  ```

  This mirrors our internal `EpisodeLabeler` data model: a list of episode dicts and baseline period dicts. The CLI will handle merging new entries into this structure. Additionally, for direct ML use, we will output a flattened CSV (`<user>_episodes.csv`) with columns `date, label, severity`. This CSV can be directly fed into the fine-tuning pipeline. For example, if the above JSON is saved, the CSV might contain:

  ```
  date,       label,      severity
  2025-03-15, depressive, 3
  2025-04-10, hypomanic,  3
  2025-04-11, hypomanic,  3
  ... (through 04-17)
  2025-05-01, baseline,   0
  ... (through 05-14)
  ```

  The `EpisodeLabeler.to_dataframe()` method already supports generating such daily records. We will ensure the CLI calls `EpisodeLabeler` under the hood to generate consistent outputs.

* **Integration with Existing Tests:** We will create new unit tests for the CLI interface (e.g., using Click’s CliRunner to simulate commands). The tests will verify that given inputs produce the expected JSON/CSV output and internal `EpisodeLabeler` state. For example, a test might call the CLI with `--date 2024-03-15 --mood hypomanic --severity 3` and then check that `EpisodeLabeler.episodes` contains one entry with `episode_type="hypomanic", severity=3`. Our existing `test_personal_calibrator.py` already tests `EpisodeLabeler.add_episode` for single and range inputs, so the CLI tests will effectively bridge from command-line text to those underlying methods. We will also test validation (e.g., that invalid dates or moods are caught). Finally, an integration test will simulate a full session (multiple labels and baseline markings) and ensure the final CSV matches expected output. This approach plugs into CI: any future changes to label schema or interface will be caught by tests if they break expected behavior.

## Data Quality Safeguards in the CLI

Ensuring high-quality labels is paramount, since clinician-provided truth will train our personalized models. We propose several safeguards inspired by data-entry best practices and known annotation workflows:

* **Double-Entry Confirmation:** For critical fields like dates, the CLI can ask for confirmation or repeated entry. For example, after typing a start/end date, the tool can echo “You entered start=2025-04-10 and end=2025-04-17, is that correct? (Y/n)”. This extra step, while optional (can be skipped with a `--no-confirm` flag for advanced users), helps catch typos early. In cases of interactive mode, we might even ask the user to re-type the date or episode label if we suspect an error (similar to password confirmations). This redundancy reduces risk of one-off mistakes.

* **Constrained Choices & Auto-Complete:** By limiting the `--mood` input to a set of choices, we prevent labeler subjectivity in naming. The CLI can support auto-completion or abbreviation (e.g., “dep” → “depressive”) to speed up input while still recording a consistent category. This also minimizes *label drift* (where one day a user might enter “Depressed” vs “Depression” another day – our tool will normalize these to the predefined categories).

* **Optional Confidence Score:** We recognize that clinicians might be unsure about certain labels. We plan to include a `--confidence <0-1>` option (or a prompt like “How confident are you in this label? (enter 1 for certain, down to 0 for guess)”). This number would be stored alongside the label (and could be used later to weight samples or flag low-confidence labels for review). If not provided, it can default to 1 (fully confident). This gives an avenue for the expert to communicate uncertainty, rather than forcing a binary decision. In practice, if a clinician is, say, only moderately sure an episode was hypomania, they might set 0.7 confidence. We will treat confidence as metadata (not currently used in model training, unless we later incorporate it for weighting).

* **Conflict Warnings (“Cool-down” Logic):** The CLI will include some domain-specific sanity checks. For instance, if a user labels a **very short episode** that contradicts clinical definitions (e.g., a “manic” episode of 2 days), the tool can warn: “Manic episodes typically last 7+ days; consider labeling as hypomanic or ensure dates are correct.” This acts as a gentle nudge to double-check. Another example: if the sensor data from our pipeline indicates no anomaly in a period labeled as severe mania, we could issue a non-blocking warning. (Concretely, if our pipeline has computed features for that date range and found nothing unusual – perhaps an API hook could let us check that – we alert: “Note: no significant change in activity/sleep during this period per sensors. Continue with mania label? (Y/n)”.) This “cool-down” is to prompt re-evaluation, not to override the human – the clinician can always proceed if they have additional context.

* **Consistency Prompts:** If the same user labels data over multiple sessions, the CLI can load previous labels and highlight potential inconsistencies. For example, if last week they labeled a certain date as “No Episode” but today try to label it as part of a depressive episode, the tool should flag this discrepancy. This is especially relevant in retrospective labeling where memory or records might conflict. By surfacing prior inputs, we encourage self-consistency. For multi-rater scenarios (in future), the tool could even show what percentage of other raters agreed on a given period (analogous to inter-annotator agreement displays: *“2 of 3 other raters marked this period as baseline”*). In enterprise tools, *“task agreement…shows the consensus between multiple annotators”* on the same item; we can adopt a simpler version: e.g., require a second pass on the labels and measure agreement with oneself (test-retest reliability) or later compare between raters.

* **Audit Log & Undo:** Each labeling action will be logged (with timestamp and rater). If a mistake is made, an `undo` subcommand or flag can remove the last-entered label. We’ll maintain an in-memory log during a session and write to a `labels_history.log` (or rely on git diffs) for an audit trail. This ensures transparency – if questions arise later (e.g., “why does the model have this training label?”) we can trace it back to who entered it and when, providing accountability and an opportunity for review.

By implementing these safeguards, we aim for **high inter-rater reliability** and **self-consistent labels**. In future iterations, we might formally measure agreement (Cohen’s kappa) if multiple clinicians label the same data, but for the MVP, these in-tool checks will already improve quality by preventing obvious errors and encouraging careful input.

## Extensibility & DevOps Considerations

To ensure the labeled data integrates smoothly into our modeling pipeline and can evolve over time, we address persistence, version control, and continuous integration:

* **Label Persistence:** All labels will be saved in the repository (or a designated data storage) in a structured format. For now, we propose storing under a `labels/` or `fixtures/` directory within the repo. For example, `labels/user123/episodes.json` (and `.csv`). Each user (or study participant) can have their own file/folder, since our personalization logic often works per-user. This mirrors the pattern already in our `PersonalCalibrator` (which expects a per-user episodes file in calibration). Storing labels as files in git means every change can be tracked and reverted if needed. We will add these files to version control (with appropriate `.gitignore` rules if needed for privacy). Since labels are typically small text, git is fine (no need for Git LFS unless we had huge label files). Each time new labels are added, collaborators can review diffs via pull requests, adding an extra layer of verification (especially important if we scale to multiple annotators).

* **Integration with Fine-Tuning Pipeline:** Our fine-tuning command (e.g. `big-mood calibrate --episodes episodes.csv ...`) will use the CSV of labels. We’ll ensure the CLI either writes to the exact path the pipeline expects (e.g., a default `episodes.csv` in a known location) or require the user to pass the path in the calibrate command. The `EpisodeLabeler.to_dataframe()` method already produces the needed format – we just need to ensure it’s invoked and saved after labeling. For example, after `label` commands, we can auto-run a `to_dataframe()` and save/update `episodes.csv`. This means as soon as the clinician finishes labeling, they can immediately run `big-mood calibrate` and the model will find the labels. In CI, we can include a test where a small dummy dataset is labeled via the CLI (perhaps simulate labeling  a couple of days as episodes), then run a truncated `calibrate` to see that it picks up those labels without errors.

* **Multi-Rater Extension:** While MVP assumes one rater at a time, we add the `--rater-id` field and include it in JSON (as shown above) to future-proof the format. In a multi-rater scenario, we might shift to storing labels in a small **SQLite database** or similar, where each label entry has rater\_id and we can combine or compare them easily via queries. For now, a simple approach is to allow separate files per rater (e.g. `labels/user123/episodes_Alice.csv` vs `episodes_Bob.csv`), and a script to merge them or compute consensus. We can define a naming/versioning convention: e.g., `episodes_user123_v1.csv` for an initial labeling, then v2, etc., or use git tags/commits to mark label revisions. Using git as the version-control for labels has the advantage that every edit is recorded with a timestamp and author (especially if each rater uses their own branch or commit signature). We will document that any manual edits to labels should be done via the CLI (to keep JSON in sync with CSV and avoid merge conflicts). We might also consider using a **GitHub issue or PR template** for label additions, so that reviews can happen if needed (though this might be overkill for now).

* **CI and Data Versioning:** Our CI pipeline (running tests) will include checks on the labeling tool (as discussed) to ensure it functions. Additionally, if we accumulate a growing set of labeled episodes (e.g., as ground-truth for fine-tuning), we should integrate those into CI in a way that if someone changes code that affects label parsing or schema, tests will fail. For example, we can keep a small “golden” label file in the repo as a fixture and test that the code can read it correctly. As the number of labels grows, we might externalize them (but still version-controlled). In terms of DevOps for deployment: since this is a CLI intended for local use, “deployment” is just packaging it with our Python library (ensuring that `make setup` or `pip install` includes this entry point). We will add documentation for users on how to use the labeling CLI as part of the README usage examples (e.g., under “Step 2: Mood Episode Labeling” we’ll update the examples to match the implemented CLI flags).

* **Incremental Label Updates:** We anticipate that labeling might be an ongoing process (new data comes in, or clinicians refine previous labels). To manage this, we suggest a **version control convention**: never delete old labels outright, but rather mark corrections. For instance, if a label is updated, one could add a field `"revised": true` or simply rely on git diff. Alternatively, maintain a changelog in the JSON (an array of events: added/removed by whom). For now, our approach will be: **append-only** labeling in the CLI (you can add new labels or mark an existing episode as removed via a special command, but the record of the removal stays in git history). We will make sure re-running the CLI on an already labeled period will warn and ask if this is a correction. This way, the history of changes is preserved.

In summary, by storing labels as code-adjacent artifacts (JSON/CSV in git) and designing the CLI to accommodate multiple raters and revisions, we ensure that our labeled ground-truth becomes a living part of the project – benefiting from the same collaboration and versioning practices as our code. This will support continuous fine-tuning: as new labels are added over weeks, models can be re-trained and evaluated, with CI possibly generating metrics (e.g., we could track “number of labeled episodes” or inter-rater kappa as a metric to know when we have sufficient agreement to retrain).

## 1-Week Implementation Plan & Success Metrics

**Goal:** Within one week, deliver a working CLI tool for mood episode labeling, integrated into our repository, with tests and documentation, enabling us to start labeling real user data for model fine-tuning immediately.

**Work Breakdown by Day/Task:**

* **Day 1-2: CLI Skeleton & Basic Functionality** – Set up the Click command group `label` and subcommands. Implement flag parsing and validation logic (date parsing, category checks). Hook up `EpisodeLabeler.add_episode` and `add_baseline` calls under the hood so that running the CLI actually populates the data structures. By end of Day 2, one should be able to do a basic label command (single or range) and see the output printed or saved (even if format is rough). *Risk:* Getting Click to handle interactive prompts gracefully might take some tweaking; mitigate by consulting Click docs/examples for prompt usage.

* **Day 3: Output Schema & File Persistence** – Implement the JSON/CSV output saving. Ensure that after each label operation, the JSON file is updated (or, if doing all labels in one session, save at session end). Write a helper to merge new labels with an existing file (so we don’t overwrite past data). Also, implement the `import` subcommand to read an existing CSV of labels (this can reuse `EpisodeLabeler` by iterating rows and calling add\_episode or add\_baseline accordingly). Begin writing unit tests for the core functions (parsing, adding, saving). *Risk:* Aligning JSON schema exactly with expectations of pipeline (minor risk, address by referencing test expectations and fine-tuning code).

* **Day 4: Interactive Mode & Safeguards** – Add the interactive prompt flow for when flags are not given. Test it manually for user experience. Implement the key safeguards: confirmation prompts (e.g., require `--confirm` or interactive yes/no after each entry), conflict checking (maintain an in-memory index of labeled dates to avoid overlaps), and basic “rule checks” (like minimum episode length warnings). This day is heavy on polishing user interaction. Also, integrate the `--rater-id` handling: e.g., ensure the output file is namespaced or contains the rater. *Risk:* Over-engineering prompts can slow input (make sure there’s an option to skip confirmations for power users). We will likely make confirmations optional or only in interactive mode to balance speed vs safety.

* **Day 5: Testing & Documentation** – Develop a comprehensive test suite: unit tests for each function (date validation, conflict detection, etc.), and CLI invocation tests for typical scenarios. Use small dummy data in tests (no need for real sensor data). Ensure tests from `test_personal_calibrator.py` still pass – if our changes affect how EpisodeLabeler is used, update tests accordingly. Write or update documentation: a section in README (or a docs markdown) with usage instructions, examples, and tips (like how to undo or how to merge multiple label files). Possibly include an **executive summary** of how this CLI fits into the workflow for new contributors (maybe in `CLINICAL_REQUIREMENTS_DOCUMENT.md`). *Risk:* None critical – just allocate sufficient time for writing clear docs and ensuring tests cover edge cases (like labeling Feb 29 on a non-leap year, etc.).

* **Day 6: Integration and Refinement** – Integrate the labeling tool with the fine-tuning pipeline end-to-end. Test by running a mini pipeline: simulate an Apple Health export (or use a small excerpt), label a known episode via CLI, run `mood-detector calibrate` to see that it ingests the labels and perhaps prints some confirmation (since actual model training might be heavy, we can stub parts if needed). Fix any bugs discovered (for example, file path issues or date format mismatches). This day also involves code cleanup (ensuring compliance with our linting/typing standards since the repo is clean and we want to keep it that way).

* **Day 7: Buffer and Review** – Use this day to address any remaining issues or nice-to-haves that were deferred. For instance, if earlier we skipped a certain safeguard due to time, attempt it now. Or, if during internal testing with a clinician colleague we find the prompts confusing, iterate on wording. Finally, prepare a short demo for the team: e.g., a recorded terminal session or a live demo script showing how a clinician would label a week of data in under a minute. This will validate our usability assumptions. *Risk:* If we’re already on track, Day 7 could also be used to begin collecting real labels as a dry run, which might reveal new feature requests (we’ll note them for future but not block the release).

**Success Metrics:** To know if this MVP is effective, we define a few measurable outcomes:

* **Functionality:** 100% of the 693 tests remain green (ensuring we didn’t break existing functionality) and new CLI tests pass, giving us confidence in reliability. Additionally, we expect high code coverage on the new module (aim for >90%, since this is critical logic).
* **Usability (Speed):** A clinician or researcher should be able to label an episode at roughly **<30 seconds per entry** via CLI. For example, labeling a month of data with 2 episodes and a baseline period (\~3 commands) should take under 2 minutes. We can measure this informally in user testing. Speed is important to prevent frustration and encourage consistent use.
* **Error Rate:** Our goal is to observe **zero critical labeling errors** in testing – e.g., no mislabeled dates or categories when following prompts. The safeguards should catch or confirm all inputs. If during internal tests we find an incorrect label got through (user meant 2025 but typed 2023 and it wasn’t caught), that’s a fail we need to fix. Success is the tool making it essentially *impossible* (or at least very unlikely) to record a wrong date or invalid category unnoticed.
* **Inter-Rater Prep:** While true inter-rater reliability will come once multiple people use it, we consider it a success criterion that the **data format supports multi-rater**. Concretely, if we have two clinicians label the same period in separate files, we can later calculate agreement (Cohen’s kappa, etc.) without having to overhaul the format. The presence of `rater_id` and structured JSON means we’re set for that. We can simulate a quick test: label the same 5 days as episodes in two files and run a small script to ensure we can compute an agreement score (not part of CLI yet, but as a check).
* **Adoption metric:** By end of week, we should be ready to label actual study data. A soft metric is **number of episodes labeled in first use** – e.g., if a clinician manages to label ≥5 historical episodes in their first session without confusion, that’s a good sign. In contrast, if they struggle to use the CLI, that’s qualitative feedback to address.

By hitting these targets, we’ll have an MVP CLI that is robust, user-friendly for our context, and already positioned for expansion. As immediate next steps after this week, we could start feeding the collected labels into our personal model fine-tuning and observe improvements in accuracy. The faster we gather high-quality labels, the sooner we can close the loop on our **Big Mood Detector** personalization – this CLI is the linchpin to move from generic research model to a tailored, clinically relevant tool. With this plan executed, we’ll be labeling mood episodes “in the wild” in no time, accelerating our iterative improve→fine-tune→evaluate cycle for the project.

**Sources:** Open-source time-series labeling tools and references were consulted for best practices and design ideas, including Geocene’s Trainset for brushing timeline labels, Label Studio’s multi-channel time-series annotation capabilities, the Grafana platform’s annotation API for time-series events, the QSL Jupyter labeling widget, and domain literature on bipolar episode tracking like the Life-Chart Method. These informed our comparative analysis and feature decisions.
